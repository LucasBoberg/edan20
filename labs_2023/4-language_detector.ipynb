{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #3: A simple language classifier with scikit-learn and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a language detector inspired and simplified from Google's _Compact language detector_, version 3 (CLD3): https://github.com/google/cld3. CLD3 is written in C++ and its code is available from GitHub. The objectives of the assignment are to:\n",
    "* Write a program to classify languages\n",
    "* Use neural networks with sklearn and PyTorch\n",
    "* Know what a classifier is\n",
    "* Write a short report of 1 to 2 pages to describe your program. You will notably comment the performance you obtained and how you could improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the GitHub description of CLD3, https://github.com/google/cld3, (_Model_ section). In your individual report you will:\n",
    "1. Summarize the system in two or three sentences;\n",
    "2. Outline the CLD3 overall architecture in a figure. Use building blocks only and do not specify the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import sys\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x17facf830>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As dataset, we will use Tatoeba, https://tatoeba.org/eng/downloads. It consists of more than 8 million short texts in 347 languages and it is available in one file called `sentences.csv`.\n",
    "\n",
    "The dataset is structured this way: There is one text per line, where each line consists of the three following fields separated by tabulations and ended by a carriage return:\n",
    "```\n",
    "sentence id [tab] language code [tab] text [cr]\n",
    "```\n",
    "Each text (sentence) has a unique id and has a language code that follows the ISO 639-3 standard (see below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope of the lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will consider six languages only: French (fra), Japanese (jpn), Chinese (cmn), English (eng), Swedish (swe), and Danish (dan). Below is an excerpt of the Tatoeba dataset limited to three languages: \n",
    "\n",
    "```\n",
    "1276    eng     Let's try something.\n",
    "1277    eng     I have to go to sleep.\n",
    "1280    eng     Today is June 18th and it is Muiriel's birthday!\n",
    "...\n",
    "1115    fra     Lorsqu'il a demandé qui avait cassé la fenêtre, tous les garçons ont pris un air innocent.\n",
    "1279    fra     Je ne supporte pas ce type.\n",
    "1441    fra     Pour une fois dans ma vie je fais un bon geste... Et ça ne sert à rien.\n",
    "...\n",
    "337413  swe     Vi trodde att det var ett flygande tefat.\n",
    "341910  swe     Detta är huset jag bodde i när jag var barn.\n",
    "341938  swe     Vi hade roligt på stranden igår.\n",
    "...\n",
    "```\n",
    "Tatoeba is updated continuously. The examples from this dataset come from a corpus your instructor downloaded on September 23, 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the ${X}$ matrix (feature matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now investigate the CLD3 features:\n",
    " *  What are the features CLD3 extracts from each text?\n",
    " * Create manually a simplified ${X}$ matrix where you will represent the 9 texts with CLD3 features. You will use a restricted set of features: You will only consider the letters _a_, _b_, and _n_ and the bigrams _an_, _ba_, and _na_. You will ignore the the rest of letters and bigrams as well as the trigrams. Your matrix will have 9 rows and 6 columns, each column will contain these counts: `[#a, #b, #n, #an, #ba, #na]`.\n",
    "\n",
    "The CLD3's original description uses relative frequencies (counts of a letter divided by the total counts of letters in the text). Here, you will use the raw counts. To help you, your instructor filled the fourth row of the matrix corresponding to the first text in French. Fill in the rest. You will __include this matrix in your report__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "-& -& -& -&-& -\\\\\n",
    "-& -& -& -&-& -\\\\\n",
    "-& -& -& -&-& -\\\\\n",
    "8& 0& 8& 1&0&0\\\\\n",
    "-& -& -& -&-& -\\\\\n",
    "-& -& -& -&-& -\\\\\n",
    "-& -& -& -&-& -\\\\\n",
    "-& -& -& -&-& -\\\\\n",
    "-& -& -& -&-& -\\\\\n",
    "\\end{bmatrix}$\n",
    "; $\\mathbf{y} =\n",
    "\\begin{bmatrix}\n",
    "     \\text{eng} \\\\\n",
    "     \\text{eng}\\\\\n",
    "     \\text{eng}\\\\\n",
    "    \\text{fra}\\\\\n",
    "   \\text{fra}  \\\\\n",
    "     \\text{fra}\\\\\n",
    "    \\text{swe}\\\\\n",
    " \\text{swe}   \\\\\n",
    " \\text{swe}   \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you check your counts, you can use the `str.count()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ngrams = ['a', 'b', 'n', 'an', 'ba', 'na']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Lorsqu'il a demandé qui avait cassé la fenêtre, tous les garçons ont pris un air innocent.\"\"\".count('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Lorsqu'il a demandé qui avait cassé la fenêtre, tous les garçons ont pris un air innocent.\"\"\".count('an')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 0, 8, 1, 0, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"\"\"Lorsqu'il a demandé qui avait cassé la fenêtre, tous les garçons ont pris un air innocent.\"\"\"\n",
    "row = []\n",
    "for ngram in example_ngrams:\n",
    "    row += [my_string.count(ngram)]\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start programming, download the Tatoeba dataset. You can use the instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-21 13:52:16--  https://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
      "Slår upp downloads.tatoeba.org (downloads.tatoeba.org)... 94.130.77.194\n",
      "Ansluter till downloads.tatoeba.org (downloads.tatoeba.org)|94.130.77.194|:443 … ansluten.\n",
      "HTTP-begäran skickad, väntar på svar... 200 OK\n",
      "Längd: 183274899 (175M) [application/octet-stream]\n",
      "Sparar till: ”sentences.tar.bz2”\n",
      "\n",
      "sentences.tar.bz2   100%[===================>] 174,78M  17,8MB/s    om 9,7s    \n",
      "\n",
      "2023-09-21 13:52:26 (18,0 MB/s) - ”sentences.tar.bz2” sparades [183274899/183274899]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://downloads.tatoeba.org/exports/sentences.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x sentences.csv\n"
     ]
    }
   ],
   "source": [
    "!tar -xvjf sentences.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code to read the dataset and split it into lines. You may have to change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\tcmn\\t我們試試看！',\n",
       " '2\\tcmn\\t我该去睡觉了。',\n",
       " '3\\tcmn\\t你在干什麼啊？',\n",
       " '4\\tcmn\\t這是什麼啊？',\n",
       " '5\\tcmn\\t今天是６月１８号，也是Muiriel的生日！',\n",
       " '6\\tcmn\\t生日快乐，Muiriel！',\n",
       " '7\\tcmn\\tMuiriel现在20岁了。',\n",
       " '8\\tcmn\\t密码是\"Muiriel\"。',\n",
       " '9\\tcmn\\t我很快就會回來。',\n",
       " '10\\tcmn\\t我不知道。']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_large = open('sentences.csv', encoding='utf8').read().strip()\n",
    "dataset_large = dataset_large.split('\\n')\n",
    "dataset_large[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size may vary as new documents are added every day to _Tatoeba_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11611245"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code to split the fields and remove possible whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'cmn', '我們試試看！'), ('2', 'cmn', '我该去睡觉了。'), ('3', 'cmn', '你在干什麼啊？')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_large = list(map(lambda x: tuple(x.split('\\t')), dataset_large))\n",
    "dataset_large = list(map(lambda x: tuple(map(str.strip, x)), dataset_large))\n",
    "dataset_large[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(map(lambda x: x[1], dataset_large))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the figures may vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eng', 1829233),\n",
       " ('rus', 1013772),\n",
       " ('ita', 863056),\n",
       " ('tur', 728016),\n",
       " ('epo', 727449),\n",
       " ('kab', 686241),\n",
       " ('ber', 649317),\n",
       " ('deu', 626486),\n",
       " ('fra', 568043),\n",
       " ('por', 422018),\n",
       " ('spa', 402305),\n",
       " ('hun', 394468),\n",
       " ('jpn', 238488),\n",
       " ('heb', 200838),\n",
       " ('ukr', 184377),\n",
       " ('nld', 176953),\n",
       " ('fin', 146641),\n",
       " ('pol', 122378),\n",
       " ('lit', 93912),\n",
       " ('mkd', 78095),\n",
       " ('tgl', 74892),\n",
       " ('cmn', 73632),\n",
       " ('ces', 73545),\n",
       " ('mar', 73277),\n",
       " ('ara', 62802),\n",
       " ('dan', 58848),\n",
       " ('tok', 53870),\n",
       " ('swe', 52960),\n",
       " ('lat', 49153),\n",
       " ('srp', 47878)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricting the Dataset to a few Languages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tatoeba dataset is very large. You will first extract a subset of it\n",
    "\n",
    "Write the code to extract texts in the languages below. For each language, you limit the number of documents to 50,000 or less if the language has less documents.\n",
    "You will call the resulting dataset: `dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['fra', 'cmn', 'jpn', 'eng', 'swe', 'dan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximal number of documents per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOCS = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a loop that:\n",
    "1. Extracts a list of all the documents in a certain language from the dataset\n",
    "2. Shuffles this list with `random.shuffle()`\n",
    "3. Adds `MAX_DOCS` to `dataset`. You just need to use a slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "dataset = []\n",
    "for lang in langs:\n",
    "    languageset = []\n",
    "    languageset += list(filter(lambda x: x[1] == lang, dataset_large))\n",
    "    random.shuffle(languageset)\n",
    "    languageset = languageset[:MAX_DOCS]\n",
    "    dataset += languageset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3912898', 'eng', \"I don't like her so much.\"),\n",
       " ('3065609', 'cmn', '你因该请一天的假期。'),\n",
       " ('3741336', 'dan', 'Du blev snydt.'),\n",
       " ('11784764', 'dan', 'Tom har absolut gehør.'),\n",
       " ('9711742', 'eng', 'Tom gave the hungry boy something to eat.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Before you can use the dataset to train a model, you need to convert it into numbers. You will carry this with out the following steps and you will write a corresponding function.\n",
    "1. You will extract the $n$-grams up to trigrams (`all_ngrams()`);\n",
    "2. Trigrams can create many symbols that most student's machines cannot process. You will reduce their numbers using hash codes (`hash_ngrams()`);\n",
    "3. You will compute the relative frequencies of the $n$-grams, replaced here by the hash codes (`calc_ref_freq()`).\n",
    "4. The results will be stored in three dictionaries, for characters, bigrams, and trigrams. You will merge these dictionaries into one (`shift_keys()`).\n",
    "\n",
    "You will then apply the functions to vectorize the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting $n$-grams\n",
    "The goal of this section is that you extract the $n$-grams from a text. By default, you will lowercase the text. The result will have the form: `[chars, bigrams, trigrams]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to extract the $n$-grams of a sentence: `ngrams(sentence, n=1, lc=True)`, `n` is a parameters. You can use list slices for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ngrams(sentence, n=1, lc=True):\n",
    "    ngrams = [sentence[idx:idx + n] for idx in range(len(sentence) - n + 1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'r', 'y', ' ', 's', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g', '.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams('try something.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tr', 'ry', 'y ', ' s', 'so', 'om', 'me', 'et', 'th', 'hi', 'in', 'ng', 'g.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams('try something.', n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use this function to extract all the $n$-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_ngrams(sentence, max_ngram=3, lc=True):\n",
    "    all_ngram_list = []\n",
    "    for i in range(1, max_ngram + 1):\n",
    "        all_ngram_list += [ngrams(sentence, n=i, lc=lc)]\n",
    "    return all_ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['t', 'r', 'y', ' ', 's', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g', '.'],\n",
       " ['tr',\n",
       "  'ry',\n",
       "  'y ',\n",
       "  ' s',\n",
       "  'so',\n",
       "  'om',\n",
       "  'me',\n",
       "  'et',\n",
       "  'th',\n",
       "  'hi',\n",
       "  'in',\n",
       "  'ng',\n",
       "  'g.'],\n",
       " ['try',\n",
       "  'ry ',\n",
       "  'y s',\n",
       "  ' so',\n",
       "  'som',\n",
       "  'ome',\n",
       "  'met',\n",
       "  'eth',\n",
       "  'thi',\n",
       "  'hin',\n",
       "  'ing',\n",
       "  'ng.']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ngrams('try something.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider languages with many characters that will make the number of bigrams and trigrams impossible to process. We will use the _hashing trick_ to reduce them, where we will gather $n$-grams into subsets using hash codes.\n",
    "\n",
    "Each item will have this format:\n",
    "`[char_hcodes, bigram_hcodes, trigram_hcodes]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a built-in hashing function that returns a unique numerical signature for a given string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-522534217003743531, 3209738916746912879, 3610042094769522055)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash('a'), hash('ab'), hash('abc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the remainder (modulo) of a division by 5, we reduce the possible codes to: 0, 1, 2, 3, or 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 0]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x % 5, (hash('a'), hash('ab'), hash('abc'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set maximal numbers for our $n$-grams using these divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARS = 521\n",
    "MAX_BIGRAMS = 1031\n",
    "MAX_TRIGRAMS = 1031"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here strings have integer codes within the range [0, `MAX_CHARS`["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[353, 342, 295]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x % MAX_CHARS, (hash('a'), hash('ab'), hash('abc'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hash codes may vary across machines and Marcus Klang wrote this function to have reproducible codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproducible_hash(string):\n",
    "    \"\"\"\n",
    "    reproducible hash on any string\n",
    "    \n",
    "    Arguments:\n",
    "       string: python string object\n",
    "    \n",
    "    Returns:\n",
    "       signed int64\n",
    "    \"\"\"\n",
    "    \n",
    "    # We are using MD5 for speed not security.\n",
    "    h = hashlib.md5(string.encode(\"utf-8\"), usedforsecurity=False)\n",
    "    return int.from_bytes(h.digest()[0:8], 'big', signed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "919145239626757800"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reproducible_hash('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reproducible_hash('a') % MAX_CHARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting $n$-grams to hash codes\n",
    "You will now convert the $n$-grams to hash codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXES = [MAX_CHARS, MAX_BIGRAMS, MAX_TRIGRAMS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `hash_ngrams` function that creates a list of hash codes from a list of $n$-grams. As arguments, you will have the list of $n$-grams `[chars, bigrams, trigrams]` as well as the list of dividers (`MAXES`).\n",
    "\n",
    "The output format will be a list of three lists:\n",
    "\n",
    "`[char_hcodes, bigram_hcodes, trigram_hcodes]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def hash_ngrams(ngrams, modulos):\n",
    "    hash_codes = [[] for _ in range(len(ngrams))]\n",
    "    for i in range(len(ngrams)):\n",
    "        for ngram in ngrams[i]:\n",
    "            hash_codes[i].append(reproducible_hash(ngram) % modulos[i])\n",
    "    return hash_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ngrams('try something.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[432, 437, 309, 86, 331, 97, 100, 32, 432, 332, 233, 310, 31, 442],\n",
       " [6, 765, 224, 203, 557, 176, 590, 711, 527, 757, 919, 57, 685],\n",
       " [848, 617, 468, 456, 873, 996, 287, 10, 817, 674, 960, 399]]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_ngrams(all_ngrams('try something.'), MAXES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Count Hash Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `calc_rel_freq(codes)` to count the codes. As in CLD3, you will return the relative frequencies.\n",
    "\n",
    "This is just an application of `Counter` to a list of codes and then a division by the length.\n",
    "\n",
    "The input is a list of codes and the output is a `Counter` object of relative frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def calc_rel_freq(codes):\n",
    "    frequencies = {}\n",
    "    for code in codes :\n",
    "        if code in frequencies:\n",
    "            frequencies[code] += 1\n",
    "        else:\n",
    "            frequencies[code] = 1\n",
    "    for key in frequencies:\n",
    "        frequencies[key] = frequencies[key]/len(codes)\n",
    "    cnt = Counter(frequencies)\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[432, 437, 309, 86, 331, 97, 100, 32, 432, 332, 233, 310, 31, 442],\n",
       " [6, 765, 224, 203, 557, 176, 590, 711, 527, 757, 919, 57, 685],\n",
       " [848, 617, 468, 456, 873, 996, 287, 10, 817, 674, 960, 399]]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_ngrams(all_ngrams('try something.'), MAXES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({432: 0.14285714285714285,\n",
       "          437: 0.07142857142857142,\n",
       "          309: 0.07142857142857142,\n",
       "          86: 0.07142857142857142,\n",
       "          331: 0.07142857142857142,\n",
       "          97: 0.07142857142857142,\n",
       "          100: 0.07142857142857142,\n",
       "          32: 0.07142857142857142,\n",
       "          332: 0.07142857142857142,\n",
       "          233: 0.07142857142857142,\n",
       "          310: 0.07142857142857142,\n",
       "          31: 0.07142857142857142,\n",
       "          442: 0.07142857142857142}),\n",
       " Counter({6: 0.07692307692307693,\n",
       "          765: 0.07692307692307693,\n",
       "          224: 0.07692307692307693,\n",
       "          203: 0.07692307692307693,\n",
       "          557: 0.07692307692307693,\n",
       "          176: 0.07692307692307693,\n",
       "          590: 0.07692307692307693,\n",
       "          711: 0.07692307692307693,\n",
       "          527: 0.07692307692307693,\n",
       "          757: 0.07692307692307693,\n",
       "          919: 0.07692307692307693,\n",
       "          57: 0.07692307692307693,\n",
       "          685: 0.07692307692307693}),\n",
       " Counter({848: 0.08333333333333333,\n",
       "          617: 0.08333333333333333,\n",
       "          468: 0.08333333333333333,\n",
       "          456: 0.08333333333333333,\n",
       "          873: 0.08333333333333333,\n",
       "          996: 0.08333333333333333,\n",
       "          287: 0.08333333333333333,\n",
       "          10: 0.08333333333333333,\n",
       "          817: 0.08333333333333333,\n",
       "          674: 0.08333333333333333,\n",
       "          960: 0.08333333333333333,\n",
       "          399: 0.08333333333333333})]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(calc_rel_freq, hash_ngrams(all_ngrams('try something.'), MAXES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the results above, we have three counter objects with numerical keys (the hash codes). You will build one dictionary of them.\n",
    "\n",
    "There is a key overlap and we must take care that a same hash code for the unigrams is not the same as in the bigrams. We will then shift the keys.\n",
    "\n",
    "The keys range from:\n",
    "1. Unigrams from 0 to 521, [0, MAX_CHARS[\n",
    "2. Bigrams from 0 to 1031, [0, MAX_BIGRAMS[\n",
    "3. Trigrams from 1 to 1031, [0, MAX_TRIGRAMS["
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will leave the unigrams keys as they are. You will shift the bigram keys by MAX_CHARS, and the trigram keys by MAX_CHARS + MAX_BIGRAMS. You can reuse the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SHIFT = []\n",
    "for i in range(len(MAXES)):\n",
    "    MAX_SHIFT += [sum(MAXES[:i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 521, 1552]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SHIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `shift_keys(dicts, MAX_SHIFT)` function that takes a list of dictionaries as input and the list of shifts and that a new unique dictionary, where the numerical keys have been shifted by the numbers in `MAX_SHIFT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def shift_keys(dicts, MAX_SHIFT):\n",
    "    new_dict = {}\n",
    "    list_dicts = list(dicts)\n",
    "    \n",
    "    for i in range(len(MAX_SHIFT)):\n",
    "        for key in list_dicts[i]:\n",
    "            new_dict[key + MAX_SHIFT[i]] = list_dicts[i][key]\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(calc_rel_freq, hash_ngrams(all_ngrams('try something.'), MAXES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_keys(map(calc_rel_freq, hash_ngrams(all_ngrams('try something.'), MAXES)), MAX_SHIFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we assemble all these utilities in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freq_dict(sentence, MAXES=MAXES, MAX_SHIFT=MAX_SHIFT):\n",
    "    hngrams = hash_ngrams(all_ngrams(sentence), MAXES)\n",
    "    fhcodes = map(calc_rel_freq, hngrams)\n",
    "    return shift_keys(fhcodes, MAX_SHIFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{432: 0.14285714285714285,\n",
       " 437: 0.07142857142857142,\n",
       " 309: 0.07142857142857142,\n",
       " 86: 0.07142857142857142,\n",
       " 331: 0.07142857142857142,\n",
       " 97: 0.07142857142857142,\n",
       " 100: 0.07142857142857142,\n",
       " 32: 0.07142857142857142,\n",
       " 332: 0.07142857142857142,\n",
       " 233: 0.07142857142857142,\n",
       " 310: 0.07142857142857142,\n",
       " 31: 0.07142857142857142,\n",
       " 442: 0.07142857142857142,\n",
       " 527: 0.07692307692307693,\n",
       " 1286: 0.07692307692307693,\n",
       " 745: 0.07692307692307693,\n",
       " 724: 0.07692307692307693,\n",
       " 1078: 0.07692307692307693,\n",
       " 697: 0.07692307692307693,\n",
       " 1111: 0.07692307692307693,\n",
       " 1232: 0.07692307692307693,\n",
       " 1048: 0.07692307692307693,\n",
       " 1278: 0.07692307692307693,\n",
       " 1440: 0.07692307692307693,\n",
       " 578: 0.07692307692307693,\n",
       " 1206: 0.07692307692307693,\n",
       " 2400: 0.08333333333333333,\n",
       " 2169: 0.08333333333333333,\n",
       " 2020: 0.08333333333333333,\n",
       " 2008: 0.08333333333333333,\n",
       " 2425: 0.08333333333333333,\n",
       " 2548: 0.08333333333333333,\n",
       " 1839: 0.08333333333333333,\n",
       " 1562: 0.08333333333333333,\n",
       " 2369: 0.08333333333333333,\n",
       " 2226: 0.08333333333333333,\n",
       " 2512: 0.08333333333333333,\n",
       " 1951: 0.08333333333333333}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_freq_dict('try something.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the Dataset\n",
    "We can now enrich the dataset with a numerical representation of the sentence. We use the utility functions and we call this new version: `dataset_num`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3912898', 'eng', \"I don't like her so much.\"),\n",
       " ('3065609', 'cmn', '你因该请一天的假期。')]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:43<00:00, 6896.52it/s] \n"
     ]
    }
   ],
   "source": [
    "dataset_num = []\n",
    "for datapoint in tqdm(dataset):\n",
    "    dataset_num += [list(datapoint) + [build_freq_dict(datapoint[2])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['3912898',\n",
       "  'eng',\n",
       "  \"I don't like her so much.\",\n",
       "  {130: 0.04,\n",
       "   86: 0.2,\n",
       "   371: 0.04,\n",
       "   97: 0.08,\n",
       "   310: 0.04,\n",
       "   323: 0.04,\n",
       "   432: 0.04,\n",
       "   15: 0.04,\n",
       "   233: 0.04,\n",
       "   514: 0.04,\n",
       "   32: 0.08,\n",
       "   332: 0.08,\n",
       "   437: 0.04,\n",
       "   331: 0.04,\n",
       "   100: 0.04,\n",
       "   69: 0.04,\n",
       "   196: 0.04,\n",
       "   442: 0.04,\n",
       "   1100: 0.041666666666666664,\n",
       "   677: 0.041666666666666664,\n",
       "   1081: 0.041666666666666664,\n",
       "   1440: 0.041666666666666664,\n",
       "   846: 0.041666666666666664,\n",
       "   778: 0.041666666666666664,\n",
       "   1492: 0.041666666666666664,\n",
       "   900: 0.041666666666666664,\n",
       "   909: 0.041666666666666664,\n",
       "   735: 0.041666666666666664,\n",
       "   1070: 0.041666666666666664,\n",
       "   995: 0.041666666666666664,\n",
       "   1027: 0.041666666666666664,\n",
       "   806: 0.041666666666666664,\n",
       "   1066: 0.08333333333333333,\n",
       "   555: 0.041666666666666664,\n",
       "   724: 0.041666666666666664,\n",
       "   1078: 0.041666666666666664,\n",
       "   795: 0.041666666666666664,\n",
       "   1466: 0.041666666666666664,\n",
       "   857: 0.041666666666666664,\n",
       "   717: 0.041666666666666664,\n",
       "   1253: 0.041666666666666664,\n",
       "   2356: 0.043478260869565216,\n",
       "   1919: 0.043478260869565216,\n",
       "   2210: 0.043478260869565216,\n",
       "   1587: 0.043478260869565216,\n",
       "   2221: 0.043478260869565216,\n",
       "   2216: 0.08695652173913043,\n",
       "   2284: 0.043478260869565216,\n",
       "   1997: 0.043478260869565216,\n",
       "   2093: 0.043478260869565216,\n",
       "   2290: 0.043478260869565216,\n",
       "   1976: 0.043478260869565216,\n",
       "   2428: 0.043478260869565216,\n",
       "   2292: 0.043478260869565216,\n",
       "   2508: 0.043478260869565216,\n",
       "   2381: 0.043478260869565216,\n",
       "   2008: 0.043478260869565216,\n",
       "   1586: 0.043478260869565216,\n",
       "   2017: 0.043478260869565216,\n",
       "   2499: 0.043478260869565216,\n",
       "   2118: 0.043478260869565216,\n",
       "   2307: 0.043478260869565216,\n",
       "   2272: 0.043478260869565216}],\n",
       " ['3065609',\n",
       "  'cmn',\n",
       "  '你因该请一天的假期。',\n",
       "  {332: 0.1,\n",
       "   138: 0.1,\n",
       "   42: 0.1,\n",
       "   419: 0.1,\n",
       "   376: 0.1,\n",
       "   210: 0.1,\n",
       "   74: 0.1,\n",
       "   236: 0.1,\n",
       "   346: 0.1,\n",
       "   157: 0.1,\n",
       "   1053: 0.1111111111111111,\n",
       "   1393: 0.1111111111111111,\n",
       "   542: 0.1111111111111111,\n",
       "   1328: 0.1111111111111111,\n",
       "   1006: 0.1111111111111111,\n",
       "   755: 0.1111111111111111,\n",
       "   1142: 0.1111111111111111,\n",
       "   914: 0.1111111111111111,\n",
       "   1046: 0.1111111111111111,\n",
       "   2380: 0.125,\n",
       "   1552: 0.125,\n",
       "   2556: 0.125,\n",
       "   2500: 0.125,\n",
       "   2538: 0.125,\n",
       "   2503: 0.125,\n",
       "   2488: 0.125,\n",
       "   2568: 0.125}]]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_num[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming: Building ${X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now build the ${X}$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLD3 architecture uses embeddings. In this lab, we will simplify it and we will use a feature vector instead consisting of the character frequencies. For example, you will represent the text:\n",
    "\n",
    "`\"Let's try something.\"`\n",
    "\n",
    "with:\n",
    "\n",
    "`{'l': 0.05, 'e': 0.1, 't': 0.15, \"'\": 0.05, 's': 0.1, ' ': 0.1, \n",
    " 'r': 0.05, 'y': 0.05, 'o': 0.05, 'm': 0.05, 'h': 0.05, 'i': 0.05, \n",
    " 'n': 0.05, 'g': 0.05, '.': 0.05}`\n",
    "\n",
    "Note that we used characters and not codes to make it more legible.\n",
    "\n",
    "To create the ${X}$ matrix, we need to transform the dictionaries of `dataset_num` into numerical vectors. The `DictVectorizer` class from the scikit-learn library, see here [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html], has two methods, `fit()` and `transform()`, and a combination of both `fit_transform()` to convert dictionaries into such vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now write the code to:\n",
    "\n",
    "1. Extract the hash code frequency dictionaries from `dataset_num` corresponding to its 3rd index;\n",
    "2. Convert the list of dictionaries into an ${X}$ matrix using `DictVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the character frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a new list of datapoints with the $n$-grams. Each item in this list will be a dictionary. You will call it `X_cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "X_cat = []\n",
    "for i in range(len(dataset_num)):\n",
    "    X_cat += [dataset_num[i][3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{130: 0.04,\n",
       " 86: 0.2,\n",
       " 371: 0.04,\n",
       " 97: 0.08,\n",
       " 310: 0.04,\n",
       " 323: 0.04,\n",
       " 432: 0.04,\n",
       " 15: 0.04,\n",
       " 233: 0.04,\n",
       " 514: 0.04,\n",
       " 32: 0.08,\n",
       " 332: 0.08,\n",
       " 437: 0.04,\n",
       " 331: 0.04,\n",
       " 100: 0.04,\n",
       " 69: 0.04,\n",
       " 196: 0.04,\n",
       " 442: 0.04,\n",
       " 1100: 0.041666666666666664,\n",
       " 677: 0.041666666666666664,\n",
       " 1081: 0.041666666666666664,\n",
       " 1440: 0.041666666666666664,\n",
       " 846: 0.041666666666666664,\n",
       " 778: 0.041666666666666664,\n",
       " 1492: 0.041666666666666664,\n",
       " 900: 0.041666666666666664,\n",
       " 909: 0.041666666666666664,\n",
       " 735: 0.041666666666666664,\n",
       " 1070: 0.041666666666666664,\n",
       " 995: 0.041666666666666664,\n",
       " 1027: 0.041666666666666664,\n",
       " 806: 0.041666666666666664,\n",
       " 1066: 0.08333333333333333,\n",
       " 555: 0.041666666666666664,\n",
       " 724: 0.041666666666666664,\n",
       " 1078: 0.041666666666666664,\n",
       " 795: 0.041666666666666664,\n",
       " 1466: 0.041666666666666664,\n",
       " 857: 0.041666666666666664,\n",
       " 717: 0.041666666666666664,\n",
       " 1253: 0.041666666666666664,\n",
       " 2356: 0.043478260869565216,\n",
       " 1919: 0.043478260869565216,\n",
       " 2210: 0.043478260869565216,\n",
       " 1587: 0.043478260869565216,\n",
       " 2221: 0.043478260869565216,\n",
       " 2216: 0.08695652173913043,\n",
       " 2284: 0.043478260869565216,\n",
       " 1997: 0.043478260869565216,\n",
       " 2093: 0.043478260869565216,\n",
       " 2290: 0.043478260869565216,\n",
       " 1976: 0.043478260869565216,\n",
       " 2428: 0.043478260869565216,\n",
       " 2292: 0.043478260869565216,\n",
       " 2508: 0.043478260869565216,\n",
       " 2381: 0.043478260869565216,\n",
       " 2008: 0.043478260869565216,\n",
       " 1586: 0.043478260869565216,\n",
       " 2017: 0.043478260869565216,\n",
       " 2499: 0.043478260869565216,\n",
       " 2118: 0.043478260869565216,\n",
       " 2307: 0.043478260869565216,\n",
       " 2272: 0.043478260869565216}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize `X_cat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert you `X_cat` matrix into a numerical representation using `DictVectorizer`: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html. You will set the `sparse` argument to False. Call the result `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "...\n",
    "v = DictVectorizer(sparse=False)\n",
    "\n",
    "X = v.fit_transform(X_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 2583)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming: Building $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now convert the list of language symbols into a $\\mathbf{y}$ vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the language symbols from `dataset_small_feat` and call the resulting list `y_cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "y_cat = []\n",
    "for i in range(len(dataset)):\n",
    "    y_cat += [dataset[i][1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eng', 'cmn', 'dan', 'dan', 'eng']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the set of language symbols and name it `y_symbols`. Then build two indices mapping the symbols to integers and the integers to symbols. Both indices will be dictionaries that you will call: `lang2idx`and `idx2lang`. Such a conversion is not necessary with sklearn. We do it because many other many machine-learning toolkits (keras or pytorch) require a numerical $\\mathbf{y}$ vector and to learn how to carry out this conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "y_symbols = set(y_cat)\n",
    "lang2idx = {lang: i for i, lang in enumerate(y_symbols)}\n",
    "idx2lang = {i: lang for i, lang in enumerate(y_symbols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'jpn', 1: 'swe', 2: 'fra', 3: 'eng', 4: 'dan', 5: 'cmn'}"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jpn': 0, 'swe': 1, 'fra': 2, 'eng': 3, 'dan': 4, 'cmn': 5}"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert your `y_cat` vector into a numerical vector. Call this vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "y = [lang2idx[lang] for lang in y_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 4, 4, 3]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming: Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a neural network using sklearn with a hidden layer of 50 nodes and a relu activation layer: https://scikit-learn.org/stable/modules/neural_networks_supervised.html. Set the maximal number of iterations to 5, in the beginning, and verbose to True. Use the default values for the rest. You will call your classifier `clf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, random_state=1234, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, random_state=1234,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, random_state=1234,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, random_state=1234,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now split the dataset into a training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We split the dataset\n",
    "We use a training set of 80% and a validation set of 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_examples, :]\n",
    "y_train = y[:training_examples]\n",
    "\n",
    "X_val = X[training_examples:, :]\n",
    "y_val = y[training_examples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.29771445\n",
      "Iteration 2, loss = 0.04065779\n",
      "Iteration 3, loss = 0.02849672\n",
      "Iteration 4, loss = 0.02339434\n",
      "Iteration 5, loss = 0.02039174\n",
      "Iteration 6, loss = 0.01842953\n",
      "Iteration 7, loss = 0.01698608\n",
      "Iteration 8, loss = 0.01582906\n",
      "Iteration 9, loss = 0.01486970\n",
      "Iteration 10, loss = 0.01425622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the `X_val` languages. You will call the result `y_val_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "y_val_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 5, 4, 4, 2, 1, 2, 5, 4, 1, 0, 5, 0, 4, 2, 3, 1, 0, 1])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 5, 4, 4, 2, 1, 2, 5, 4, 1, 0, 5, 0, 4, 2, 3, 1, 0, 1]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `accuracy_score()` function to evaluate your model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9923"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "accuracy_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         jpn       1.00      1.00      1.00     10046\n",
      "         swe       0.99      0.98      0.98      9905\n",
      "         fra       1.00      1.00      1.00      9964\n",
      "         eng       1.00      1.00      1.00     10043\n",
      "         dan       0.98      0.99      0.98      9864\n",
      "         cmn       1.00      1.00      1.00     10178\n",
      "\n",
      "    accuracy                           0.99     60000\n",
      "   macro avg       0.99      0.99      0.99     60000\n",
      "weighted avg       0.99      0.99      0.99     60000\n",
      "\n",
      "Micro F1: 0.9923\n",
      "Macro F1 0.9922400771568108\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_val_pred, target_names=y_symbols))\n",
    "print('Micro F1:', f1_score(y_val, y_val_pred, average='micro'))\n",
    "print('Macro F1', f1_score(y_val, y_val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10018,     0,     0,     0,     0,    28],\n",
       "       [    0,  9680,     9,     5,   211,     0],\n",
       "       [    0,     3,  9944,    11,     6,     0],\n",
       "       [    0,    13,    15, 10009,     6,     0],\n",
       "       [    0,   115,     1,    16,  9729,     3],\n",
       "       [    9,     2,     0,     5,     4, 10158]])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may try to increase the number of iterations to improve the score. You may also try change the parameters of the multilayer percetron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the language of a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will predict the languages of the strings below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"Salut les gars !\", \"Hejsan grabbar!\", \"Hello guys!\", \"Hejsan tjejer!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{56: 0.0625,\n",
       " 234: 0.125,\n",
       " 15: 0.125,\n",
       " 69: 0.0625,\n",
       " 432: 0.0625,\n",
       " 86: 0.1875,\n",
       " 32: 0.0625,\n",
       " 331: 0.125,\n",
       " 31: 0.0625,\n",
       " 437: 0.0625,\n",
       " 333: 0.0625,\n",
       " 1209: 0.06666666666666667,\n",
       " 640: 0.06666666666666667,\n",
       " 582: 0.06666666666666667,\n",
       " 1542: 0.06666666666666667,\n",
       " 1492: 0.06666666666666667,\n",
       " 900: 0.06666666666666667,\n",
       " 739: 0.06666666666666667,\n",
       " 1319: 0.06666666666666667,\n",
       " 1238: 0.13333333333333333,\n",
       " 982: 0.06666666666666667,\n",
       " 1415: 0.06666666666666667,\n",
       " 557: 0.06666666666666667,\n",
       " 1161: 0.06666666666666667,\n",
       " 1020: 0.06666666666666667,\n",
       " 2362: 0.07142857142857142,\n",
       " 1608: 0.07142857142857142,\n",
       " 2199: 0.07142857142857142,\n",
       " 2349: 0.07142857142857142,\n",
       " 2284: 0.07142857142857142,\n",
       " 1958: 0.07142857142857142,\n",
       " 1720: 0.07142857142857142,\n",
       " 1925: 0.07142857142857142,\n",
       " 2370: 0.07142857142857142,\n",
       " 2546: 0.07142857142857142,\n",
       " 1752: 0.07142857142857142,\n",
       " 1805: 0.07142857142857142,\n",
       " 1670: 0.07142857142857142,\n",
       " 2269: 0.07142857142857142}"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_freq_dict('Salut les gars !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create features vectors from this list. Call this matrix `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "X_tester = []\n",
    "for i in range(len(docs)):\n",
    "    X_tester += [build_freq_dict(docs[i])]\n",
    "\n",
    "t = DictVectorizer(sparse=False)\n",
    "X_test = t.fit_transform(X_tester)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.125     , 0.        , 0.0625    , 0.0625    , 0.0625    ,\n",
       "        0.0625    , 0.1875    , 0.        , 0.        , 0.125     ,\n",
       "        0.        , 0.        , 0.125     , 0.0625    , 0.        ,\n",
       "        0.0625    , 0.0625    , 0.06666667, 0.        , 0.06666667,\n",
       "        0.        , 0.        , 0.        , 0.06666667, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.06666667, 0.        , 0.        , 0.        , 0.06666667,\n",
       "        0.06666667, 0.06666667, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.06666667, 0.        , 0.06666667, 0.        ,\n",
       "        0.        , 0.13333333, 0.        , 0.06666667, 0.        ,\n",
       "        0.06666667, 0.06666667, 0.        , 0.        , 0.06666667,\n",
       "        0.        , 0.        , 0.07142857, 0.07142857, 0.        ,\n",
       "        0.        , 0.        , 0.07142857, 0.07142857, 0.        ,\n",
       "        0.        , 0.        , 0.07142857, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07142857, 0.        , 0.07142857,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07142857, 0.        , 0.07142857,\n",
       "        0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.07142857,\n",
       "        0.        ],\n",
       "       [0.        , 0.13333333, 0.06666667, 0.06666667, 0.        ,\n",
       "        0.        , 0.06666667, 0.        , 0.06666667, 0.2       ,\n",
       "        0.        , 0.06666667, 0.06666667, 0.06666667, 0.06666667,\n",
       "        0.        , 0.13333333, 0.07142857, 0.        , 0.        ,\n",
       "        0.        , 0.07142857, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07142857, 0.        , 0.        , 0.07142857,\n",
       "        0.        , 0.        , 0.        , 0.07142857, 0.        ,\n",
       "        0.07142857, 0.        , 0.07142857, 0.        , 0.07142857,\n",
       "        0.        , 0.        , 0.07142857, 0.        , 0.07142857,\n",
       "        0.07142857, 0.        , 0.        , 0.        , 0.07142857,\n",
       "        0.        , 0.        , 0.07142857, 0.07142857, 0.        ,\n",
       "        0.07692308, 0.        , 0.        , 0.        , 0.07692308,\n",
       "        0.07692308, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07692308, 0.        , 0.07692308, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.07692308,\n",
       "        0.07692308, 0.        , 0.07692308, 0.        , 0.        ,\n",
       "        0.07692308, 0.        , 0.        , 0.07692308, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.07692308, 0.07692308, 0.        , 0.07692308, 0.        ,\n",
       "        0.        ],\n",
       "       [0.18181818, 0.        , 0.09090909, 0.09090909, 0.        ,\n",
       "        0.09090909, 0.09090909, 0.09090909, 0.        , 0.        ,\n",
       "        0.09090909, 0.        , 0.09090909, 0.09090909, 0.09090909,\n",
       "        0.        , 0.        , 0.        , 0.1       , 0.        ,\n",
       "        0.1       , 0.        , 0.1       , 0.        , 0.        ,\n",
       "        0.1       , 0.        , 0.        , 0.1       , 0.        ,\n",
       "        0.        , 0.1       , 0.        , 0.        , 0.        ,\n",
       "        0.1       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1       , 0.        , 0.1       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.11111111, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.11111111,\n",
       "        0.11111111, 0.        , 0.        , 0.        , 0.11111111,\n",
       "        0.        , 0.        , 0.        , 0.11111111, 0.        ,\n",
       "        0.        , 0.11111111, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.11111111, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.11111111, 0.        , 0.        ,\n",
       "        0.11111111],\n",
       "       [0.        , 0.        , 0.        , 0.21428571, 0.        ,\n",
       "        0.        , 0.07142857, 0.        , 0.21428571, 0.07142857,\n",
       "        0.        , 0.07142857, 0.07142857, 0.07142857, 0.07142857,\n",
       "        0.07142857, 0.07142857, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07692308, 0.        , 0.        , 0.07692308,\n",
       "        0.        , 0.        , 0.15384615, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07692308, 0.15384615, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.07692308, 0.07692308,\n",
       "        0.        , 0.        , 0.07692308, 0.        , 0.07692308,\n",
       "        0.07692308, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07692308, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.08333333, 0.        , 0.08333333, 0.        ,\n",
       "        0.        , 0.08333333, 0.        , 0.08333333, 0.        ,\n",
       "        0.08333333, 0.08333333, 0.        , 0.        , 0.08333333,\n",
       "        0.        , 0.        , 0.        , 0.08333333, 0.08333333,\n",
       "        0.08333333, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.08333333,\n",
       "        0.08333333, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the prediction that you will store in a variable called `pred_languages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 96 features, but MLPClassifier is expecting 2583 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucas/Developer/python/edan20/labs_2023/4-language_detector.ipynb Cell 157\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucas/Developer/python/edan20/labs_2023/4-language_detector.ipynb#Y312sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Write your code here\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucas/Developer/python/edan20/labs_2023/4-language_detector.ipynb#Y312sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pred_languages \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1160\u001b[0m, in \u001b[0;36mMLPClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Predict using the multi-layer perceptron classifier.\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \n\u001b[1;32m   1149\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[39m    The predicted classes.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict(X)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1164\u001b[0m, in \u001b[0;36mMLPClassifier._predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, X, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1163\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Private predict method with optional input validation\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pass_fast(X, check_input\u001b[39m=\u001b[39mcheck_input)\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1167\u001b[0m         y_pred \u001b[39m=\u001b[39m y_pred\u001b[39m.\u001b[39mravel()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:207\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._forward_pass_fast\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Predict using the trained model\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[39mThis is the same as _forward_pass but does not record the activations\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39m    The decision function of the samples for each class in the model.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m check_input:\n\u001b[0;32m--> 207\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(X, accept_sparse\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m], reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m \u001b[39m# Initialize first layer\u001b[39;00m\n\u001b[1;32m    210\u001b[0m activation \u001b[39m=\u001b[39m X\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:625\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 625\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39mreset)\n\u001b[1;32m    627\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:414\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 414\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    415\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    416\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 96 features, but MLPClassifier is expecting 2583 features as input."
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "pred_languages = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 4])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model with PyTorch\n",
    "You will now recreate a PyTorch model with the same architecture as in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "Create a model identical to the one you created with sklearn. Use the same activation function for the hidden layer and no activation in the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=2583, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = X.shape[1]\n",
    "model = Model(input_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the loss `loss_fn` and optimizer `optimizer`. As optimizer, use the same as in sklearn. See here: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here. (The solution is given)\n",
    "loss_fn = nn.CrossEntropyLoss()    # cross entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "X_val = torch.Tensor(X_val)\n",
    "y_val = torch.LongTensor(y_val)\n",
    "\n",
    "X_test = torch.Tensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=2583, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit your network on your training set. Write a code similar to that seen during the lecture and use five epochs to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765.704064067424\n",
      "174.51052177065867\n",
      "140.88319964771836\n",
      "122.14930617929895\n",
      "108.1038764459961\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "for epoch in range(5):\n",
    "    loss_train = 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        ...\n",
    "    print(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the validation set `X_val` in the form of logits. Call the result: `Y_val_pred_logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=2583, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "Y_val_pred_logits = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -5.9929,   6.8918,  -3.0228,  -9.0067,  -4.4374,  -9.8311],\n",
       "        [ -5.7582, -10.4759, -12.6710,  -3.3666,  18.9000, -12.0623],\n",
       "        [ -6.9647,  -9.3725, -22.0371,  22.4224,  -8.4151, -14.8630],\n",
       "        [ -8.7401, -11.8870,  -8.4524,  -2.2943,  16.8711,  -7.5894],\n",
       "        [ -3.7173,  -5.8539,  -5.3409,  -9.7889,  -6.0550,  11.3400]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val_pred_logits[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the validation set `X_val` in the form of probabilities. Use `torch.softmax()` for that and call the result: `Y_val_pred_proba`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "Y_val_pred_proba = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5364e-06, 9.9994e-01, 4.9445e-05, 1.2455e-07, 1.2017e-05, 5.4616e-08],\n",
       "        [1.9547e-11, 1.7467e-13, 1.9448e-14, 2.1365e-10, 1.0000e+00, 3.5748e-14],\n",
       "        [1.7271e-13, 1.5547e-14, 4.9143e-20, 1.0000e+00, 4.0500e-14, 6.4144e-17],\n",
       "        [7.5369e-12, 3.2398e-13, 1.0050e-11, 4.7487e-09, 1.0000e+00, 2.3820e-11]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val_pred_proba[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the categories from the probabilities in `Y_val_pred_proba`. Use the `torch.argmax()` function. Call the result `y_val_pred`. Check that the prediction corresponds to the real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "y_val_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 3, 4, 5, 1, 2, 0, 2, 3, 4, 1, 0, 5, 0, 0, 5, 5, 0, 0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 3, 4, 5, 1, 2, 0, 2, 3, 4, 1, 0, 5, 0, 0, 5, 5, 0, 0])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         dan       0.98      0.98      0.98     10081\n",
      "         swe       0.98      0.98      0.98     10001\n",
      "         eng       0.99      1.00      1.00      9989\n",
      "         jpn       1.00      1.00      1.00      9868\n",
      "         cmn       1.00      1.00      1.00     10027\n",
      "         fra       1.00      1.00      1.00     10034\n",
      "\n",
      "    accuracy                           0.99     60000\n",
      "   macro avg       0.99      0.99      0.99     60000\n",
      "weighted avg       0.99      0.99      0.99     60000\n",
      "\n",
      "Micro F1: 0.9922333333333333\n",
      "Macro F1 0.9922547719497999\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_val_pred, target_names=y_symbols))\n",
    "print('Micro F1:', f1_score(y_val, y_val_pred, average='micro'))\n",
    "print('Macro F1', f1_score(y_val, y_val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9914,   146,    20,     0,     0,     1],\n",
       "       [  167,  9815,    15,     0,     1,     3],\n",
       "       [   15,     6,  9962,     0,     0,     6],\n",
       "       [    0,     0,     0,  9839,    29,     0],\n",
       "       [    3,     2,     1,    14, 10005,     2],\n",
       "       [    6,     5,    24,     0,     0,  9999]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict your languages with PyTorch. Reuse `X_test` and call the result `Y_test_pred_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "Y_test_pred_proba = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8276e-07, 6.6974e-07, 1.4123e-07, 1.6559e-09, 2.8414e-08, 1.0000e+00],\n",
       "        [4.1081e-04, 9.9952e-01, 2.4307e-05, 9.6759e-07, 4.6090e-05, 1.8566e-06],\n",
       "        [3.9380e-02, 4.1912e-05, 9.4139e-01, 7.8594e-06, 8.5014e-04, 1.8331e-02],\n",
       "        [9.9818e-01, 1.8032e-03, 7.3723e-08, 9.3494e-07, 1.2838e-05, 9.8474e-07]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the probabilities, extract the predicted languages and map them to strings. Call the results `pred_languages_pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "pred_languages_pytorch = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fra', 'swe', 'eng', 'dan']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_languages_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning in your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will:\n",
    "1. Write a short individual report on your program. Do not forget to:\n",
    "   * Summarize CLD3 and outline its architecture\n",
    "   * Identify the features used by CLD3\n",
    "   * Describe your architecture and tell how it is different from CLD3\n",
    "   * Include the feature matrix you computed manually\n",
    "   * Outline the differences between sklearn and PyTorch\n",
    "\n",
    "Submit your report as well as your notebook (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, you can either\n",
    "1. Write directly your text in Canvas, or\n",
    "2. Use Latex and Overleaf (www.overleaf.com). This will probably help you structure your text. You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is October 6, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postscript from Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created this assignment from an examination I wrote in 2019 for the course on applied machine learning. I simplified it from the `README.md` on GitHub, https://github.com/google/cld3. I found the C++ code difficult to understand and I reimplemented a Keras/Tensorflow version of it from this `README`. Should you be interested, you can find it here: https://github.com/pnugues/language-detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
